{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":182,"outputs":[{"output_type":"stream","text":"/kaggle/input/Sanoop_Resume_.txt\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import spacy","execution_count":183,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"/kaggle/input/Sanoop_Resume_.txt\") as f:\n    text=f.read()","execution_count":184,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text","execution_count":185,"outputs":[{"output_type":"execute_result","execution_count":185,"data":{"text/plain":"'SANOOP PUSHPAJAN\\n\\nPh: 8792718609\\nEmail ID:sanooppushpajan838@gmail.com\\n\\nCareer Objective:\\nSeeking a quality environment that will serve as a platform to learn and enhance my\\nskills where on my knowledge and experience can be shared and enriched. To work in a\\nprogressive and stimulating work culture with the freedom to be creative, innovative and to\\nexcel and grow the organizations area of work.\\n\\nTechnical Skill:\\nAnalytics techniques: Machine Learning models,(Regression, Classification, Segmentation,\\nTime series Analysis) ,Webscraping(Beautiful Soup),Data Mining, Data exploratory Analysis,\\nData Manipulation, Feature Engineering.\\nLanguages: Python, SQL.\\nIDE : Jupyter Notebook, SQL Developer, VS Code.\\nOperating System: Windows 7,Windows 8.\\n\\nWork Experience:\\n•\\n\\nWorked in TechMahindra as a Software Engineer implementing Machine\\nLearning from Aug 2016 to Feb 2019.\\n\\n•\\n\\n2.5+ years Total experience in IT including more than 1.5+ year experience in Data\\nAnalytics and Machine Learning.\\n\\n•\\n\\nCurrently working as a Data Scientist in Bytewave Digital from March 2019 to till\\ndate.\\n\\nProfessional Summary:\\n•\\n•\\n•\\n•\\n•\\n•\\n\\n1.5+ years of Data Analytics and Machine learning experience using Python.\\nExcellent knowledge and hands on experience in Machine Learning techniques\\nlike Regression, Classification , Segmentation and Time series Analysis.\\nExperience in Database like Oracle 10g, Oracle 11g.\\nExperience in Webscraping using Beautiful soup and Python UI using T-kinter.\\nConversant with all stages of Software Development Life Cycle: Analysis,\\nDesign, Development, Implementation, Testing and Documentation.\\nExposure to working in a global delivery team.\\n\\nEDUCATION\\nBachelor of Engineering | NMAM Institute of Technology, Mangalore\\nFROM 2012-2016\\n•\\n•\\n•\\n\\nGraduated from NMAM Institute of technology with CGPA – 7.9.\\nHigher Secondary with 76% from Bharatiya Vidya Bhavan , Year of passing 2016.\\nHigh School with 89% from Bharatiya Vidya Bhavan , Year of passing 2010.\\n\\n\\x0cPROJECTS SUMMARY.\\nCLASSIC-Machine Learning Implementation (DEC 2017-Present):\\nEnvironment: Jupyter Notebook, SQL Developer.\\nLanguage: Python 3.6,SQL.\\nTeam size:15\\nDescription: SOE Classic is a strategic delivery system for different BT Global Services\\nproducts. Classic receives the order from the ordering systems (upstream systems) and fulfils\\nthe order through its service delivery workflow process. Classic acts upstream system to BFG\\n(Big Friendly Giant) which is a BTGS Central Repository for Service Assurance. It is an\\norder management, workflow and trouble ticketing application based on the Clarify CRM\\nCOTS product suite. Classic is the Standard Operating Environment (SOE) system used by\\nBT Global Services to provision BT MPLS/ harmonized circuits and routers.\\nResponsibilities:\\n• Implementing and testing out different machine learning models to find out the best\\nmodel for higher accuracy.\\n• Extracting the dependent variables and independent variables for creating the dataset.\\n• Working as a team member to Solve out the issues during the modelling.\\n• Analyzing different machine learning techniques to perfect the working model.\\n• Teaching the other members in the project about different machine learning techniques.\\n\\nDCMO: Working in DCMO Project as a Data Analyst (Jan –Dec 2017):\\nTo provide Data Management Solutions to customer by Loading and amending data in the\\nBFG. A dedicated resource in DCMO Project, worked closely with different customers by\\nproviding them data management solutions. Bulk Data load requests come from BTGS\\ncontract user with approval from respective contract owner’s to feed the new site and\\ninventory data in BFG.\\nProject Domain\\nProject Name:\\n\\nTelecom\\nGS_DASM_FP_GS-STRADA ASM_A\\n\\nFINTECH AND IMI-CCI (March 2019 – Present)\\n•\\n•\\n•\\n\\nPracticed Webscraping using Beautiful soup for creating a Security Master for Fintech.\\nPracticed on developing UI models for Machine learning using T-kinter.\\nStudies conducted on implimenting time-series analysis for projects for client IMI-CCI.\\n\\nCERTIFICATIONS:\\n•\\n•\\n\\nGoogle Cloud Platform Big Data and Machine Learning Fundamentals from Coursera.\\n\\n•\\n\\nSuccessfully completed ’Machine learning A-Z :Hands on Data science with Python and R from\\nUdemy.\\n\\nCompleted ‘Leveraging unstructured data with cloud Data proc on google cloud platform’ from\\nCoursera.\\n\\n\\x0c•\\n\\nSuccessfully completed ‘Serverless Data Analysis with Google BigQuery and Cloud DataFlow’\\nfrom Coursera .\\n\\nAWARDS AND ACHIEVEMENTS.\\nReceived 3 E-cards as a mark of appreciation from\\nClient(British Telecommunications).\\n• Received ‘pat on the back award’ for implementing Random\\nforest model in the project.\\n\\nPERSONAL PROFILE:\\nFather’s Name : PUSHPAJAN.\\nMother’s Name: SAJITHA PUSHPAJAN.\\nDate of Birth : 12th March 1994.\\nSex\\n: Male\\nMarital Status :Single.\\nLanguages Known : English, Hindi,Malayalam.\\nHobbies: Gym, Football, Listening music and watching movies.\\nPermanent Address: Souparnika House, Stadium Road\\nPO Irinave,Kannur.PIN:670301.\\n\\nI hereby declare that the above furnished details are true and to best of my knowledge.\\n(SANOOP PUSHPAJAN)\\n\\n\\x0c'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy as sp","execution_count":186,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = sp.load(\"en_core_web_sm\")","execution_count":187,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc=nlp(text)","execution_count":210,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for token in doc:\n  #  print(token.text)","execution_count":211,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for ent in doc.ents:\n  #  print(ent.text,ent.start_char,ent.end_char,ent.label_)","execution_count":212,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re","execution_count":213,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from spacy.matcher import Matcher","execution_count":214,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matcher = Matcher(nlp.vocab)\n","execution_count":215,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pattern = [{\"LOWER\": \"experience\"}]","execution_count":222,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"matcher.add(\"sampath\", None, pattern)","execution_count":223,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"phrase_matches=matcher(doc)","execution_count":224,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sentences=list(doc.sents)","execution_count":225,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(phrase_matches)","execution_count":227,"outputs":[{"output_type":"stream","text":"[(1423779801226798880, 36, 37), (1423779801226798880, 139, 140), (1423779801226798880, 169, 170), (1423779801226798880, 178, 179), (1423779801226798880, 233, 234), (1423779801226798880, 243, 244), (1423779801226798880, 261, 262), (1423779801226798880, 274, 275)]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for sent in doc.sents:\n    doc_sentence = nlp(str(sent))\n    matches = matcher(doc_sentence)\n    if matches:\n        print (matches,doc_sentence, '\\n')","execution_count":229,"outputs":[{"output_type":"stream","text":"[(1423779801226798880, 26, 27)] Career Objective:\nSeeking a quality environment that will serve as a platform to learn and enhance my\nskills where on my knowledge and experience can be shared and enriched. \n\n[(1423779801226798880, 1, 2)] Work Experience:\n \n\n[(1423779801226798880, 1, 2), (1423779801226798880, 10, 11)] Total experience in IT including more than 1.5+ year experience in Data\nAnalytics and Machine Learning.\n\n \n\n[(1423779801226798880, 11, 12)] •\n\n1.5+ years of Data Analytics and Machine learning experience using Python.\n \n\n[(1423779801226798880, 5, 6)] Excellent knowledge and hands on experience in Machine Learning techniques\nlike Regression, Classification , Segmentation and Time series Analysis.\n \n\n[(1423779801226798880, 0, 1)] Experience in Database like Oracle \n\n[(1423779801226798880, 0, 1)] Experience in Webscraping using Beautiful soup and Python UI using T-kinter.\n \n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"##for match_id, start,end in matches:\n    #string_id = nlp.vocab.strings[match_id]  # Get string representation\n    #span = doc[start:end]  # The matched span\n    #print(sentences)","execution_count":126,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}